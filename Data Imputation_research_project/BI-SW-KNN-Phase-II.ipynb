{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from sklearn.impute import KNNImputer\n",
    "import array\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions here\n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             DateTime  Sensor_Data  Feature_1  Feature_2  Actual_Data\n",
      "0     3/10/2004 0:00          1.2         62         77          1.2\n",
      "1     3/10/2004 1:00          1.0         62         76          1.0\n",
      "2     3/10/2004 2:00          0.9         45         60          0.9\n",
      "3     3/10/2004 3:00          0.6          1          2          0.6\n",
      "4     3/10/2004 4:00          1.0         21         34          1.0\n",
      "...              ...          ...        ...        ...          ...\n",
      "9370  4/4/2005 10:00          3.1        472        190          3.1\n",
      "9371  4/4/2005 11:00          2.4        353        179          2.4\n",
      "9372  4/4/2005 12:00          2.4        293        175          2.4\n",
      "9373  4/4/2005 13:00          2.1        235        156          2.1\n",
      "9374  4/4/2005 14:00          2.2        265        168          2.2\n",
      "\n",
      "[9375 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load csv and print header and values\n",
    "\n",
    "df = pd.read_csv('pems_output_air_quality.csv',header=0)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "<ipython-input-145-8dcc0eacb77c>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_simplified[\"Hour_of_Day\"] = pd.to_numeric(df_simplified[\"Hour_of_Day\"]) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_Id      1    2    3    4    5    6    7    8    9    10   ...  91   92   \\\n",
      "Hour_of_Day                                                    ...             \n",
      "1            1.2  1.2  1.7  2.7  2.9  1.8  2.1  1.7  2.3  2.0  ...  1.3  1.6   \n",
      "2            1.0  1.0  NaN  NaN  NaN  1.8  1.2  1.2  1.4  1.6  ...  0.7  1.0   \n",
      "3            0.9  0.9  1.4  1.6  2.5  1.8  0.8  0.9  1.0  0.9  ...  0.5  0.9   \n",
      "4            0.6  0.6  0.8  1.7  2.4  1.1  0.7  0.7  0.7  0.7  ...  0.4  0.5   \n",
      "5            1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.3  0.5   \n",
      "6            0.7  0.7  0.6  1.0  1.2  1.0  0.6  0.5  0.6  0.5  ...  0.5  0.5   \n",
      "7            0.7  0.7  0.8  1.2  1.0  1.4  0.9  0.5  0.7  0.7  ...  1.4  1.0   \n",
      "8            1.1  1.1  1.4  1.5  0.9  2.2  1.3  1.6  1.5  1.5  ...  3.3  3.5   \n",
      "9            2.0  2.0  4.4  2.7  1.4  5.5  3.4  4.1  4.7  4.8  ...  5.8  6.4   \n",
      "10           2.2  2.2  1.0  3.7  1.6  8.1  3.7  6.6  6.6  6.2  ...  5.0  2.9   \n",
      "11           1.7  1.7  3.1  3.2  2.2  5.8  5.3  4.3  4.5  4.0  ...  3.5  3.4   \n",
      "12           1.5  1.5  2.7  4.1  2.8  4.2  4.1  2.9  2.8  3.3  ...  2.9  2.8   \n",
      "13           1.6  1.6  2.1  3.6  2.8  3.1  3.3  2.5  2.2  2.8  ...  2.5  2.8   \n",
      "14           1.9  1.9  2.5  2.8  2.0  2.9  4.0  2.8  2.2  3.0  ...  2.3  2.8   \n",
      "15           2.9  2.9  2.7  2.0  1.8  2.9  3.8  2.6  2.3  3.3  ...  2.4  2.3   \n",
      "16           2.2  2.2  2.9  2.0  1.9  2.5  2.8  2.0  2.2  3.5  ...  2.6  1.8   \n",
      "17           2.2  2.2  2.8  2.5  3.0  2.3  2.9  2.9  2.8  4.0  ...  2.2  2.0   \n",
      "18           2.9  2.9  2.4  2.3  2.9  2.8  2.9  2.5  2.7  4.6  ...  3.5  3.5   \n",
      "19           2.6  4.8  3.9  3.2  2.5  6.1  3.4  5.0  3.7  4.1  ...  3.9  4.9   \n",
      "20           2.0  6.9  3.7  4.2  4.6  8.0  3.9  7.6  5.1  4.5  ...  4.5  4.6   \n",
      "21           2.2  6.1  6.6  4.2  5.9  6.5  3.2  6.7  5.1  3.9  ...  3.8  3.4   \n",
      "22           2.2  3.9  4.4  4.2  3.4  4.2  5.1  5.7  3.2  4.0  ...  2.8  2.7   \n",
      "23           1.6  1.5  3.5  3.1  2.1  3.2  2.6  2.8  2.1  2.2  ...  1.7  1.9   \n",
      "24           1.2  1.0  5.4  2.6  2.2  1.4  1.7  2.6  1.7  2.1  ...  1.2  1.5   \n",
      "\n",
      "Date_Id      93   94   95   96   97   98   99   100  \n",
      "Hour_of_Day                                          \n",
      "1            1.0  1.5  1.3  2.0  0.7  1.2  1.9  2.5  \n",
      "2            0.7  1.1  0.9  1.0  0.5  0.7  1.4  2.1  \n",
      "3            1.0  0.6  0.6  0.6  0.3  0.5  0.8  1.0  \n",
      "4            0.6  0.5  0.5  0.6  0.3  0.4  0.6  0.8  \n",
      "5            1.0  0.4  0.5  0.5  1.0  0.4  0.6  1.0  \n",
      "6            0.9  0.6  0.9  0.4  0.3  0.5  0.5  0.5  \n",
      "7            1.2  1.0  0.7  0.4  0.7  1.0  1.0  1.3  \n",
      "8            2.9  4.6  0.6  0.4  1.5  4.4  2.7  3.6  \n",
      "9            4.8  4.8  1.3  0.4  1.8  4.0  5.0  4.8  \n",
      "10           4.3  4.0  1.9  0.5  1.4  2.4  3.3  3.7  \n",
      "11           3.1  1.0  2.1  0.6  1.5  2.1  2.4  1.9  \n",
      "12           2.4  2.0  1.8  0.7  1.2  2.7  1.7  1.7  \n",
      "13           2.8  2.0  1.8  0.7  1.2  2.8  1.7  1.9  \n",
      "14           2.5  2.0  1.6  0.6  1.3  2.3  2.4  1.8  \n",
      "15           2.6  1.9  1.2  0.5  1.0  2.0  2.2  1.7  \n",
      "16           2.3  1.9  1.2  0.6  1.4  2.6  2.2  1.8  \n",
      "17           2.4  1.9  1.4  0.8  1.6  2.7  2.3  1.7  \n",
      "18           3.5  2.6  1.3  0.8  2.7  3.1  2.6  2.8  \n",
      "19           4.8  3.2  1.1  1.0  2.3  3.1  2.3  3.6  \n",
      "20           5.1  3.4  1.1  1.0  1.5  3.6  3.3  3.3  \n",
      "21           4.7  3.4  1.1  1.0  1.4  2.9  3.5  2.9  \n",
      "22           2.4  2.9  1.8  0.9  1.0  2.2  2.5  1.7  \n",
      "23           2.5  2.7  2.3  0.7  1.1  2.0  2.5  1.7  \n",
      "24           2.7  1.3  2.6  0.8  1.2  2.1  2.4  2.8  \n",
      "\n",
      "[24 rows x 100 columns]\n",
      "     Sensor_Data        Date  Hour_of_Day  Date_Id  Window_Id\n",
      "49           NaN  2004-03-12            2        3        NaN\n",
      "73           NaN  2004-03-13            2        4        NaN\n",
      "97           NaN  2004-03-14            2        5        NaN\n",
      "274          NaN  2004-03-21           11       12        NaN\n",
      "298          NaN  2004-03-22           11       13        NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-8dcc0eacb77c>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_missing[\"Window_Id\"] = np.nan\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Window_Id  Hour_of_Day  Date_Id  Sensor_Data        DateTime  Feature_1  \\\n",
      "0         1.0          2.0      1.0          1.0  3/10/2004 1:00         62   \n",
      "1         1.0          2.0      2.0          1.0  3/11/2004 1:00         62   \n",
      "2         1.0          2.0      3.0          NaN  3/12/2004 1:00        133   \n",
      "3         1.0          2.0      4.0          NaN  3/13/2004 1:00        139   \n",
      "4         1.0          2.0      5.0          NaN  3/14/2004 1:00        174   \n",
      "5         1.0          2.0      6.0          1.8  3/15/2004 1:00        106   \n",
      "6         1.0          2.0      7.0          1.2  3/16/2004 1:00         79   \n",
      "7         1.0          2.0      8.0          1.2  3/17/2004 1:00         95   \n",
      "8         1.0          2.0      9.0          1.4  3/18/2004 1:00         92   \n",
      "9         1.0          2.0     10.0          1.6  3/19/2004 1:00        103   \n",
      "10        1.0          2.0     11.0          1.6  3/20/2004 1:00         86   \n",
      "11        1.0          2.0     12.0          2.1  3/21/2004 1:00        133   \n",
      "12        1.0          2.0     13.0          1.5  3/22/2004 1:00         74   \n",
      "13        1.0          2.0     14.0          0.8  3/23/2004 1:00         47   \n",
      "14        1.0          2.0     15.0          1.1  3/24/2004 1:00         76   \n",
      "15        1.0          2.0     16.0          1.0  3/25/2004 1:00         42   \n",
      "16        1.0          2.0     17.0          1.4  3/26/2004 1:00         79   \n",
      "17        1.0          2.0     18.0          1.0  3/27/2004 1:00         70   \n",
      "18        1.0          2.0     19.0          2.3  3/28/2004 1:00        111   \n",
      "19        1.0          2.0     20.0          0.6  3/29/2004 1:00         21   \n",
      "20        1.0          2.0     21.0          0.7  3/30/2004 1:00         41   \n",
      "21        1.0          2.0     22.0          1.0  3/31/2004 1:00         44   \n",
      "22        1.0          2.0     23.0          1.2   4/1/2004 1:00         85   \n",
      "23        1.0          2.0     24.0          1.3   4/2/2004 1:00         84   \n",
      "24        1.0          2.0     25.0          1.3   4/3/2004 1:00         73   \n",
      "\n",
      "    Feature_2  Actual_Data        Date  Forward_Impute  Backward_Impute  \n",
      "0          76          1.0  2004-03-10             1.0         1.000000  \n",
      "1          76          1.0  2004-03-11             1.0         1.000000  \n",
      "2         110          1.9  2004-03-12             1.0         1.133333  \n",
      "3          97          1.9  2004-03-13             1.0         1.133333  \n",
      "4         119          2.8  2004-03-14             1.0         1.133333  \n",
      "5          93          1.8  2004-03-15             1.8         1.800000  \n",
      "6          88          1.2  2004-03-16             1.2         1.200000  \n",
      "7          90          1.2  2004-03-17             1.2         1.200000  \n",
      "8         107          1.4  2004-03-18             1.4         1.400000  \n",
      "9          96          1.6  2004-03-19             1.6         1.600000  \n",
      "10         83          1.6  2004-03-20             1.6         1.600000  \n",
      "11         90          2.1  2004-03-21             2.1         2.100000  \n",
      "12         76          1.5  2004-03-22             1.5         1.500000  \n",
      "13         64          0.8  2004-03-23             0.8         0.800000  \n",
      "14         79          1.1  2004-03-24             1.1         1.100000  \n",
      "15         58          1.0  2004-03-25             1.0         1.000000  \n",
      "16         91          1.4  2004-03-26             1.4         1.400000  \n",
      "17         79          1.0  2004-03-27             1.0         1.000000  \n",
      "18         97          2.3  2004-03-28             2.3         2.300000  \n",
      "19         31          0.6  2004-03-29             0.6         0.600000  \n",
      "20         66          0.7  2004-03-30             0.7         0.700000  \n",
      "21         64          1.0  2004-03-31             1.0         1.000000  \n",
      "22         70          1.2  2004-04-01             1.2         1.200000  \n",
      "23         93          1.3  2004-04-02             1.3         1.300000  \n",
      "24         88          1.3  2004-04-03             1.3         1.300000  \n",
      "    Window_Id  Hour_of_Day  Date_Id  Sensor_Data         DateTime  Feature_1  \\\n",
      "0         2.0         11.0      1.0          1.7  3/10/2004 10:00        112   \n",
      "1         2.0         11.0      2.0          1.7  3/11/2004 10:00        112   \n",
      "2         2.0         11.0      3.0          3.1  3/12/2004 10:00        187   \n",
      "3         2.0         11.0      4.0          3.2  3/13/2004 10:00        250   \n",
      "4         2.0         11.0      5.0          2.2  3/14/2004 10:00        144   \n",
      "5         2.0         11.0      6.0          5.8  3/15/2004 10:00        394   \n",
      "6         2.0         11.0      7.0          5.3  3/16/2004 10:00        396   \n",
      "7         2.0         11.0      8.0          4.3  3/17/2004 10:00        280   \n",
      "8         2.0         11.0      9.0          4.5  3/18/2004 10:00        349   \n",
      "9         2.0         11.0     10.0          4.0  3/19/2004 10:00        253   \n",
      "10        2.0         11.0     11.0          2.6  3/20/2004 10:00        166   \n",
      "11        2.0         11.0     12.0          NaN  3/21/2004 10:00        123   \n",
      "12        2.0         11.0     13.0          NaN  3/22/2004 10:00        115   \n",
      "13        2.0         11.0     14.0          3.4  3/23/2004 10:00        237   \n",
      "14        2.0         11.0     15.0          3.4  3/24/2004 10:00        218   \n",
      "15        2.0         11.0     16.0          2.3  3/25/2004 10:00        133   \n",
      "16        2.0         11.0     17.0          3.1  3/26/2004 10:00        232   \n",
      "17        2.0         11.0     18.0          2.1  3/27/2004 10:00        129   \n",
      "18        2.0         11.0     19.0          2.3  3/28/2004 10:00        128   \n",
      "19        2.0         11.0     20.0          1.5  3/29/2004 10:00        119   \n",
      "20        2.0         11.0     21.0          4.7  3/30/2004 10:00        320   \n",
      "21        2.0         11.0     22.0          1.7  3/31/2004 10:00        144   \n",
      "22        2.0         11.0     23.0          2.8   4/1/2004 10:00        192   \n",
      "23        2.0         11.0     24.0          3.3   4/2/2004 10:00        196   \n",
      "24        2.0         11.0     25.0          1.0   4/3/2004 10:00          1   \n",
      "25        2.0         11.0     26.0          1.0   4/4/2004 10:00          1   \n",
      "26        2.0         11.0     27.0          3.1   4/5/2004 10:00        236   \n",
      "27        2.0         11.0     28.0          2.9   4/6/2004 10:00        182   \n",
      "28        2.0         11.0     29.0          1.6   4/7/2004 10:00        112   \n",
      "29        2.0         11.0     30.0          2.6   4/8/2004 10:00        189   \n",
      "30        2.0         11.0     31.0          2.4   4/9/2004 10:00        190   \n",
      "31        2.0         11.0     32.0          1.9  4/10/2004 10:00        124   \n",
      "32        2.0         11.0     33.0          1.6  4/11/2004 10:00         87   \n",
      "\n",
      "    Feature_2  Actual_Data_x        Date  Forward_Impute  Backward_Impute  \\\n",
      "0          98            1.7  2004-03-10        1.700000         1.700000   \n",
      "1          98            1.7  2004-03-11        1.700000         1.700000   \n",
      "2         122            3.1  2004-03-12        3.100000         3.100000   \n",
      "3         126            3.2  2004-03-13        3.200000         3.200000   \n",
      "4          98            2.2  2004-03-14        2.200000         2.200000   \n",
      "5         157            5.8  2004-03-15        5.800000         5.800000   \n",
      "6         150            5.3  2004-03-16        5.300000         5.300000   \n",
      "7         134            4.3  2004-03-17        4.300000         4.300000   \n",
      "8         150            4.5  2004-03-18        4.500000         4.500000   \n",
      "9         149            4.0  2004-03-19        4.000000         4.000000   \n",
      "10        107            2.6  2004-03-20        2.600000         2.600000   \n",
      "11         93            1.7  2004-03-21        3.090909         1.866667   \n",
      "12         83            1.8  2004-03-22        3.272727         1.866667   \n",
      "13        132            3.4  2004-03-23        3.400000         3.400000   \n",
      "14        113            3.4  2004-03-24        3.400000         3.400000   \n",
      "15         99            2.3  2004-03-25        2.300000         2.300000   \n",
      "16        119            3.1  2004-03-26        3.100000         3.100000   \n",
      "17         93            2.1  2004-03-27        2.100000         2.100000   \n",
      "18         94            2.3  2004-03-28        2.300000         2.300000   \n",
      "19         97            1.5  2004-03-29        1.500000         1.500000   \n",
      "20        159            4.7  2004-03-30        4.700000         4.700000   \n",
      "21        118            1.7  2004-03-31        1.700000         1.700000   \n",
      "22        113            2.8  2004-04-01        2.800000         2.800000   \n",
      "23        127            3.3  2004-04-02        3.300000         3.300000   \n",
      "24          2            1.0  2004-04-03        1.000000         1.000000   \n",
      "25          2            1.0  2004-04-04        1.000000         1.000000   \n",
      "26        124            3.1  2004-04-05        3.100000         3.100000   \n",
      "27        107            2.9  2004-04-06        2.900000         2.900000   \n",
      "28         96            1.6  2004-04-07        1.600000         1.600000   \n",
      "29        113            2.6  2004-04-08        2.600000         2.600000   \n",
      "30        119            2.4  2004-04-09        2.400000         2.400000   \n",
      "31         90            1.9  2004-04-10        1.900000         1.900000   \n",
      "32         77            1.6  2004-04-11        1.600000         1.600000   \n",
      "\n",
      "    Actual_Data_y  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "5             NaN  \n",
      "6             NaN  \n",
      "7             NaN  \n",
      "8             NaN  \n",
      "9             NaN  \n",
      "10            NaN  \n",
      "11            NaN  \n",
      "12            NaN  \n",
      "13            NaN  \n",
      "14            NaN  \n",
      "15            NaN  \n",
      "16            NaN  \n",
      "17            NaN  \n",
      "18            NaN  \n",
      "19            NaN  \n",
      "20            NaN  \n",
      "21            NaN  \n",
      "22            NaN  \n",
      "23            NaN  \n",
      "24            NaN  \n",
      "25            NaN  \n",
      "26            NaN  \n",
      "27            NaN  \n",
      "28            NaN  \n",
      "29            NaN  \n",
      "30            NaN  \n",
      "31            NaN  \n",
      "32            NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\users\\gurud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Assign K value here\n",
    "\n",
    "KNNImputation_value = 15\n",
    "\n",
    "# Reduce the dataset to use 100 days of the data\n",
    "df = df[:2400]\n",
    "\n",
    "# pick datetime and sensor data with missing values\n",
    "\n",
    "df_simplified = df[['DateTime', 'Sensor_Data']]\n",
    "\n",
    "# Split the datetime column into date column and hour column\n",
    "\n",
    "df_simplified.loc[:,\"Date\"] = pd.to_datetime(df_simplified['DateTime']).dt.date\n",
    "df_simplified.loc[:,\"Hour_of_Day\"] = pd.to_datetime(df_simplified['DateTime']).dt.strftime(\"%H\")\n",
    "df_simplified[\"Hour_of_Day\"] = pd.to_numeric(df_simplified[\"Hour_of_Day\"]) + 1\n",
    "\n",
    "# Generate a unique number for each date for simplicity of the program \n",
    "df_simplified.loc[:,\"Date_Id\"] = df_simplified.groupby(['Date'], sort=False).ngroup() + 1\n",
    "\n",
    "# Do the same for original table by making sure date and hour columns are separte we will use these columns\n",
    "# later for joining\n",
    "\n",
    "df.loc[:,\"Date\"] = pd.to_datetime(df['DateTime']).dt.date\n",
    "df.loc[:,\"Hour_of_Day\"] = pd.to_datetime(df['DateTime']).dt.strftime(\"%H\")\n",
    "df[\"Hour_of_Day\"] = pd.to_numeric(df[\"Hour_of_Day\"]) + 1\n",
    "df.loc[:,\"Date_Id\"] = df.groupby(['Date'], sort=False).ngroup() + 1\n",
    "\n",
    "# Now that we have split datetime into date and hour we will drop datetime column\n",
    "df_simplified = df_simplified.drop(['DateTime'],axis=1)\n",
    "\n",
    "# Pivot the table by transforming datetime row into column this way we have a new column for each date is generated for 24 hours of the day\n",
    "# so the values of each column represent sensor data for each hour\n",
    "df_pivot = df_simplified.pivot(index='Hour_of_Day', columns='Date_Id', values='Sensor_Data')\n",
    "print(df_pivot)\n",
    "# Filter all missing sensor data value into a separate data frameFind all missing values\n",
    "df_missing = df_simplified[df_simplified['Sensor_Data'].isna()] \n",
    "df_missing[\"Window_Id\"] = np.nan\n",
    "\n",
    "# We need to loop thru the each missing value and create window. Each window will have one or more missing values along with the sensor data \n",
    "# before and after the missing value(s). If number of missing values within in the window is less then or equal 5, we take 5 sensor values before and 5 sensor \n",
    "# values after the missing value(s). However, if there are more than 5 missing values for example if there are 10 missing values it will take \n",
    "# 10 values before and 10 values after the missing values(s).\n",
    "\n",
    "# min window imputing size is set to 5 as per above explaination\n",
    "min_window_imputing_size = 20\n",
    "\n",
    "# Create an empty data frame which will later be used to form windows\n",
    "df_windows = pd.DataFrame(columns=['Window_Id', 'Hour_of_Day', 'Date_Id', 'Sensor_Data'])\n",
    "window_id = 0\n",
    "print(df_missing)\n",
    "for index, row in df_missing.iterrows():\n",
    "    \n",
    "    col_index = row['Date_Id'] - 1\n",
    "    row_index = row['Hour_of_Day'] - 1\n",
    "   \n",
    "    # check if the previous missing value is NaN, this check will help add all consecutive missing values into one window. \n",
    "    # When previous missing value is NaN then it means a window is already build in the previous iteration\n",
    "    if (isNaN(df_pivot.iat[row_index,col_index-1])):\n",
    "        continue\n",
    "        \n",
    "    # check if current value is not null\n",
    "    #if(isNaN(df_pivot.iat[row_index,col_index]) == False):\n",
    "        #continue\n",
    "        \n",
    "    counter = 1\n",
    "    while True:\n",
    "        # In the pivot table check if the next day for same hour has a NaN values if it is then continue to loop to find total consecutive \n",
    "        # missing values else break\n",
    "        if(isNaN(df_pivot.iat[row_index,col_index + counter])):\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # Find the total size of the window\n",
    "    no_of_imputation_in_gap = min_window_imputing_size if counter <= min_window_imputing_size else counter\n",
    "    \n",
    "    # Find window start index\n",
    "    start_index_of_window = (col_index - no_of_imputation_in_gap) if (col_index - no_of_imputation_in_gap) > 0 else 0       \n",
    "    \n",
    "    # Find window end index\n",
    "    end_index_of_window = (col_index + counter + no_of_imputation_in_gap) if (col_index + counter + no_of_imputation_in_gap) < (df_pivot.shape[1] -1) else df_pivot.shape[1] -1\n",
    "    \n",
    "    # From the pivot table, filter the data frame to fetch window which contains both missing values and sensor data to use for imputation  \n",
    "    ds_missing = df_pivot.iloc[row_index, start_index_of_window : end_index_of_window]\n",
    "    \n",
    "    # Update window column with a unique window id so later this column is used to filter to fetch window specific sensore data\n",
    "    window_id = window_id + 1\n",
    "    df_missing.loc[index, 'Window_Id'] = window_id\n",
    "    \n",
    "    # Create a new dataframe df_windows which holds all window data\n",
    "    for date_id, value in ds_missing.items():\n",
    "        df_windows = df_windows.append({'Window_Id': window_id, 'Hour_of_Day': row['Hour_of_Day'], 'Date_Id': date_id, 'Sensor_Data': value}, ignore_index=True)\n",
    "\n",
    "# Now that we have all the windows data in df_windows, next part of the code will loop thru each window to impute data\n",
    "\n",
    "# Create new dataframe df_merged which is an inner join of df_windows and original df dataframe to get some additional columns\n",
    "# Merge will rename the columns if both the merged dataframe has same column names and we need to correct them and drop additionals\n",
    "df_merged = pd.merge(df_windows, df, on=['Hour_of_Day', 'Date_Id'], how='inner')\n",
    "\n",
    "df_merged = df_merged.drop(['Sensor_Data_y'], axis=1)\n",
    "df_merged.rename(columns = {'Sensor_Data_x':'Sensor_Data'}, inplace = True)\n",
    "   \n",
    "# impute_column_name will hold the column name of the sensor data to impute\n",
    "impute_column_name = \"Sensor_Data\"\n",
    "\n",
    "# Create two new columns Forward_Impute, Backward_Impute which initially has same data as sensor data column but further these columns will \n",
    "# populate itself with the forward imputed and backward imputed values\n",
    "df_merged[\"Forward_Impute\"] = df_merged[impute_column_name]\n",
    "df_merged[\"Backward_Impute\"] = df_merged[impute_column_name]\n",
    "\n",
    "\n",
    "# Loop each window\n",
    "for i in range(1,window_id + 1):\n",
    "   \n",
    "    # Filter data specific to the window, this wil fetch all the records of a specific window which includes both NaN and sensor data before \n",
    "    # and after gap\n",
    "    df_window = df_merged[df_merged['Window_Id'] == i]\n",
    "    # Since the df_window now has filter records, its index column of dataframe will not be ordinal, reset will help get is reordered\n",
    "    df_window.reset_index(drop=True, inplace=True)\n",
    "   \n",
    "    # declare some variables to used in the inner loop\n",
    "    \n",
    "    # Hour of the day\n",
    "    window_hour = df_window['Hour_of_Day'].values[0]\n",
    "    \n",
    "    # First Date Id of the window\n",
    "    first_date_id = df_window['Date_Id'].values[0]\n",
    "  \n",
    "    # Last Date Id of the window\n",
    "    last_date_id = df_window['Date_Id'].values[-1]\n",
    "    \n",
    "    # Find the first occurrence of NaN in the window\n",
    "    first_missing_occurrence_date_id = 0\n",
    "    for index, row in df_window.iterrows():\n",
    "        if(isNaN(row[impute_column_name])):\n",
    "            first_missing_occurrence_date_id = row[\"Date_Id\"]\n",
    "            break;\n",
    "        else:\n",
    "            continue;\n",
    "        \n",
    "    # Find total missing values\n",
    "    total_missing_values = df_window[impute_column_name].isna().sum()\n",
    "    \n",
    "    # Last missing occurance of NaN\n",
    "    last_missing_occurrence_date_id = first_missing_occurrence_date_id + total_missing_values - 1\n",
    "    \n",
    "    \n",
    "    # Forward Imputation - Loop for the sliding window for Forward imputation and populate the imputed values into \"Forward_Impute\" column\n",
    "    for j in range(0,total_missing_values):\n",
    "        \n",
    "        # filter to fetch the sliding window from the main window. \n",
    "        # Sliding window in the forward impute will start from the begining and impute one missing value at a time so each sliding window\n",
    "        # will have a missing sensor value at the end of the window\n",
    "        df_window_subset = df_window[(df_window['Date_Id'] >= (first_date_id + j)) & (df_window['Date_Id'] <= (first_missing_occurrence_date_id + j))]\n",
    "        cols = ['Date_Id','Forward_Impute', 'Feature_1', 'Feature_2']\n",
    "        \n",
    "        # From the sliding window, create a new dataframe to be used by KNN algorithm which requires feature columns and no Nan values in \n",
    "        # the data. So those are replaced by 0\n",
    "        df_knn_window_subset = df_window_subset[cols].copy()\n",
    "        df_knn_window_subset[cols] = df_knn_window_subset[cols].fillna(0).astype(int)\n",
    "        df_knn_window_subset[cols] = df_knn_window_subset[cols].replace({'0':np.nan, 0:np.nan})\n",
    "        \n",
    "        # Perform KNN imputation on the resultant dataframe \"df_knn_window_subset\"\n",
    "        # Define imputer\n",
    "        imputer = KNNImputer(n_neighbors=KNNImputation_value, weights='uniform', metric='nan_euclidean')\n",
    "        # fit on the dataset\n",
    "        imputer.fit(df_knn_window_subset)\n",
    "        # transform the dataset\n",
    "        df_knn_window_subset_transformed = imputer.transform(df_knn_window_subset)\n",
    "        df_result = pd.DataFrame(df_knn_window_subset_transformed, columns =cols)\n",
    "        \n",
    "        # Retrieve the imputed value from resultant data frame and update df_window\n",
    "        # Missing row index\n",
    "        missing_row_index = first_missing_occurrence_date_id - first_date_id\n",
    "        df_window.at[missing_row_index+j, 'Forward_Impute'] = df_result.at[missing_row_index,\"Forward_Impute\"]\n",
    "        j=j+1\n",
    "    \n",
    "    \n",
    "    # Backward Imputation - Loop for the sliding window for Forward imputation and populate the imputed values into \"Backward_Impute\" column    \n",
    "    for j in range(0,total_missing_values):\n",
    "        \n",
    "        # filter to fetch the sliding window from the main window. \n",
    "        # Sliding window in the backward impute will start from the end and impute one missing value at a time to the center so each sliding window\n",
    "        # will have a missing sensor value at the begining of the window\n",
    "        df_window_subset = df_window[(df_window['Date_Id'] <= (last_date_id - j)) & (df_window['Date_Id'] >= (last_missing_occurrence_date_id - j))]\n",
    "        cols = ['Date_Id','Backward_Impute', 'Feature_1', 'Feature_2']\n",
    "        df_knn_window_subset = df_window_subset[cols].copy()  \n",
    "        df_knn_window_subset[cols] = df_knn_window_subset[cols].fillna(0).astype(int)\n",
    "        df_knn_window_subset[cols] = df_knn_window_subset[cols].replace({'0':np.nan, 0:np.nan})\n",
    "        \n",
    "        # Perform KNN imputation on the resultant dataframe \"df_knn_window_subset\"\n",
    "        # Define imputer\n",
    "        imputer = KNNImputer(n_neighbors=KNNImputation_value, weights='uniform', metric='nan_euclidean')\n",
    "        # fit on the dataset\n",
    "        imputer.fit(df_knn_window_subset)\n",
    "        # transform the dataset\n",
    "        df_knn_window_subset_transformed = imputer.transform(df_knn_window_subset)\n",
    "        df_result = pd.DataFrame(df_knn_window_subset_transformed, columns =cols)\n",
    "        \n",
    "        # Retrieve the imputed value from result data frame and update df_window\n",
    "        # Missing row index\n",
    "        missing_row_index = 0\n",
    "        df_window.at[last_missing_occurrence_date_id-first_date_id - j, 'Backward_Impute'] = df_result.at[missing_row_index,\"Backward_Impute\"]\n",
    "        \n",
    "\n",
    "    # After forward and backward sliding, we will have columns \"Forward_Impute\", Backward_Impute\" populated with imputed values\n",
    "    # Now merge those values into df_merged table\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        print( df_window)\n",
    "    df_merged = pd.merge(df_merged, df_window, on=['Hour_of_Day', 'Date_Id'], how='left')\n",
    "   \n",
    "    df_merged['Backward_Impute_x'] = df_merged['Backward_Impute_y'].fillna(df_merged['Backward_Impute_x'])\n",
    "    df_merged['Forward_Impute_x'] = df_merged['Forward_Impute_y'].fillna(df_merged['Forward_Impute_x'])\n",
    "    drop_cols = ['Window_Id_y', 'Sensor_Data_y', 'DateTime_y', 'Feature_1_y', 'Feature_2_y', 'Date_y', 'Forward_Impute_y', 'Backward_Impute_y']\n",
    "    df_merged = df_merged.drop(drop_cols, axis=1)    \n",
    "    # Rename columns created with merge\n",
    "    df_merged.rename(columns = {'Window_Id_x':'Window_Id', 'Sensor_Data_x':'Sensor_Data', 'DateTime_x':'DateTime', 'Feature_1_x':'Feature_1','Feature_2_x':'Feature_2', 'Date_x':'Date', 'Forward_Impute_x':'Forward_Impute', 'Backward_Impute_x':'Backward_Impute'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     1.9\n",
      "3     1.9\n",
      "4     2.8\n",
      "36    1.7\n",
      "37    1.8\n",
      "Name: Actual_Data, dtype: float64\n",
      "2     1.066667\n",
      "3     1.066667\n",
      "4     1.066667\n",
      "36    2.478788\n",
      "37    2.569697\n",
      "Name: Prediction, dtype: float64\n",
      "R2 Score:\n",
      "-6.096798627716949\n",
      "\n",
      " Root Mean Square Error:\n",
      "1.0575705478729025\n",
      "\n",
      " MAE:\n",
      "0.9896969696969695\n",
      "    Actual_Data  Prediction\n",
      "2           1.9    1.066667\n",
      "3           1.9    1.066667\n",
      "4           2.8    1.066667\n",
      "36          1.7    2.478788\n",
      "37          1.8    2.569697\n",
      "    Forward_Impute  Backward_Impute  Sensor_Data  Prediction  Prediction\n",
      "0         1.000000         1.000000          1.0         NaN    0.000000\n",
      "1         1.000000         1.000000          1.0         NaN    0.000000\n",
      "2         1.000000         1.133333          NaN    1.066667    1.066667\n",
      "3         1.000000         1.133333          NaN    1.066667    1.066667\n",
      "4         1.000000         1.133333          NaN    1.066667    1.066667\n",
      "5         1.800000         1.800000          1.8         NaN    0.000000\n",
      "6         1.200000         1.200000          1.2         NaN    0.000000\n",
      "7         1.200000         1.200000          1.2         NaN    0.000000\n",
      "8         1.400000         1.400000          1.4         NaN    0.000000\n",
      "9         1.600000         1.600000          1.6         NaN    0.000000\n",
      "10        1.600000         1.600000          1.6         NaN    0.000000\n",
      "11        2.100000         2.100000          2.1         NaN    0.000000\n",
      "12        1.500000         1.500000          1.5         NaN    0.000000\n",
      "13        0.800000         0.800000          0.8         NaN    0.000000\n",
      "14        1.100000         1.100000          1.1         NaN    0.000000\n",
      "15        1.000000         1.000000          1.0         NaN    0.000000\n",
      "16        1.400000         1.400000          1.4         NaN    0.000000\n",
      "17        1.000000         1.000000          1.0         NaN    0.000000\n",
      "18        2.300000         2.300000          2.3         NaN    0.000000\n",
      "19        0.600000         0.600000          0.6         NaN    0.000000\n",
      "20        0.700000         0.700000          0.7         NaN    0.000000\n",
      "21        1.000000         1.000000          1.0         NaN    0.000000\n",
      "22        1.200000         1.200000          1.2         NaN    0.000000\n",
      "23        1.300000         1.300000          1.3         NaN    0.000000\n",
      "24        1.300000         1.300000          1.3         NaN    0.000000\n",
      "25        1.700000         1.700000          1.7         NaN    0.000000\n",
      "26        1.700000         1.700000          1.7         NaN    0.000000\n",
      "27        3.100000         3.100000          3.1         NaN    0.000000\n",
      "28        3.200000         3.200000          3.2         NaN    0.000000\n",
      "29        2.200000         2.200000          2.2         NaN    0.000000\n",
      "30        5.800000         5.800000          5.8         NaN    0.000000\n",
      "31        5.300000         5.300000          5.3         NaN    0.000000\n",
      "32        4.300000         4.300000          4.3         NaN    0.000000\n",
      "33        4.500000         4.500000          4.5         NaN    0.000000\n",
      "34        4.000000         4.000000          4.0         NaN    0.000000\n",
      "35        2.600000         2.600000          2.6         NaN    0.000000\n",
      "36        3.090909         1.866667          NaN    2.478788    2.478788\n",
      "37        3.272727         1.866667          NaN    2.569697    2.569697\n",
      "38        3.400000         3.400000          3.4         NaN    0.000000\n",
      "39        3.400000         3.400000          3.4         NaN    0.000000\n",
      "40        2.300000         2.300000          2.3         NaN    0.000000\n",
      "41        3.100000         3.100000          3.1         NaN    0.000000\n",
      "42        2.100000         2.100000          2.1         NaN    0.000000\n",
      "43        2.300000         2.300000          2.3         NaN    0.000000\n",
      "44        1.500000         1.500000          1.5         NaN    0.000000\n",
      "45        4.700000         4.700000          4.7         NaN    0.000000\n",
      "46        1.700000         1.700000          1.7         NaN    0.000000\n",
      "47        2.800000         2.800000          2.8         NaN    0.000000\n",
      "48        3.300000         3.300000          3.3         NaN    0.000000\n",
      "49        1.000000         1.000000          1.0         NaN    0.000000\n",
      "50        1.000000         1.000000          1.0         NaN    0.000000\n",
      "51        3.100000         3.100000          3.1         NaN    0.000000\n",
      "52        2.900000         2.900000          2.9         NaN    0.000000\n",
      "53        1.600000         1.600000          1.6         NaN    0.000000\n",
      "54        2.600000         2.600000          2.6         NaN    0.000000\n",
      "55        2.400000         2.400000          2.4         NaN    0.000000\n",
      "56        1.900000         1.900000          1.9         NaN    0.000000\n",
      "57        1.600000         1.600000          1.6         NaN    0.000000\n"
     ]
    }
   ],
   "source": [
    "# \"Forward_Impute\", \"Backward_Impute\" are further averaged into \"Prediction\" column\n",
    "\n",
    "\n",
    "\n",
    "df_merged_actual= pd.merge(df_merged,df[['DateTime','Actual_Data']],on='DateTime', how='inner')\n",
    "df_merged[\"Transform_Prediction\"] = df_merged[\"Sensor_Data\"].apply(lambda x: x if x!=x else 0)\n",
    "df_merged['Prediction'] = df_merged[\"Transform_Prediction\"].fillna((df_merged['Forward_Impute'] + df_merged['Backward_Impute'])/2)\n",
    "\n",
    "df_merged_actual['Actual_Data'] = df_merged[\"Transform_Prediction\"].fillna(df_merged_actual['Actual_Data'])\n",
    "\n",
    "\n",
    "\n",
    "# Acutal data and Predictions columns are concatenated into df_Results\n",
    "y_true = df_merged_actual[df_merged_actual['Actual_Data'] != 0]['Actual_Data']\n",
    "y_pred = df_merged[df_merged['Prediction'] != 0]['Prediction']\n",
    "\n",
    "\n",
    "# Calculate R2 score\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print(\"R2 Score:\")\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "# Calculate RMSE\n",
    "\n",
    "MSE = np.square(np.subtract(y_true,y_pred)).mean() \n",
    " \n",
    "RMSE = math.sqrt(MSE)\n",
    "print(\"\\n Root Mean Square Error:\")\n",
    "print(RMSE)\n",
    "\n",
    "# Calculate MAE\n",
    "\n",
    "print(\"\\n MAE:\")\n",
    "MAE = mean_absolute_error(y_true, y_pred)\n",
    "print(MAE)\n",
    "\n",
    "# Concatenating all the results\n",
    "df_Results = pd.concat([y_true, y_pred], axis=1)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_Results)\n",
    "    \n",
    "# All columns\n",
    "df_all_result = pd.concat([df_Results, df_merged], axis=1)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_all_result[['Forward_Impute', 'Backward_Impute', 'Sensor_Data', 'Prediction']])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
